#!/bin/env python

import sys
import json
import yaml
import argparse
import logging
from dataspec import Loader, preprocess_spec
import dataspec.template_engines as engines
import dataspec.outputs as outputs
import dataspec.types as types
import dataspec.key_providers as key_providers
from dataspec import utils
from dataspec import SpecException

log = logging.getLogger(__name__)


def main():
    parser = argparse.ArgumentParser(description='Run dataspec.')
    group = parser.add_argument_group('input')
    group.add_argument('-s', '--spec', help='Spec to Use')
    group.add_argument('--inline', help='Spec as string')
    parser.add_argument('-i', '--iterations', default=100, type=int,
                        help='Number of Iterations to Execute')
    parser.add_argument('-o', '--outdir',
                        help='Output directory')
    parser.add_argument('-p', '--outfile-prefix', dest='outfileprefix', default='generated',
                        help='Prefix for output files, default is generated')
    parser.add_argument('-e', '--extension', default='',
                        help='Extension to add to generated files')
    parser.add_argument('-t', '--template',
                        help='Path to template to populate')
    parser.add_argument('-r', '--records-per-file', dest='recordsperfile', default=sys.maxsize, type=int,
                        help='Number of records to place in each file, default is all, requires -o to be specified')
    parser.add_argument('-k', '--printkey', action='store_true', default=False,
                        help='When printing to stdout field name should be printed along with value')
    parser.add_argument('-c', '--code', nargs='+',
                        help='Path to custom defined functions in one or more modules to load')
    parser.add_argument('-d', '--datadir',
                        help='Path to external directory to load external data files such as csvs')
    parser.add_argument('-l', '--log-level', dest='log_level', default=logging.INFO,
                        help='Logging level verbosity, default is info, valid are "debug","info","warn","error","off"')
    parser.add_argument('-f', '--format', default=None,
                        help='Formatter for output records, default is none, valid are: ' + str(types.valid_formats()))
    parser.add_argument('--strict', action='store_true', default=False,
                        help='Enforce schema validation for all registered field specs')
    parser.add_argument('--debug-spec', dest='debug_spec', action='store_true', default=False,
                        help='Debug spec after internal reformatting')

    try:
        args = parser.parse_args()

        _configure_logging(args)
        log.info('Starting Loading Configurations...')
        log.debug("Parsing Args")

        if args.code:
            log.debug('Loading custom code from %s', args.code)
            for code in args.code:
                utils.load_custom_code(code)

        log.debug('Attempting to load Data Spec from %s', args.spec if args.spec else args.inline)
        spec = _load_spec(args)
        if args.debug_spec:
            print(json.dumps(preprocess_spec(spec), indent=4))
            return
        loader = Loader(spec, args.datadir, enforce_schema=args.strict)

        output = _configure_output(args)

        keys = key_providers.from_spec(loader.specs)

        log.info('Starting Processing...')
        for i in range(0, args.iterations):
            for key in keys.get():
                value = loader.get(key).next(i)
                output.handle(key, value)
            output.finished_record()
        log.info('Finished Processing')
    except SpecException as e:
        log.error(str(e))


def _configure_output(args):
    if args.outdir:
        log.debug('Creating output file writer for dir: %s', args.outdir)
        writer = outputs.FileWriter(
            outdir=args.outdir,
            outname=args.outfileprefix,
            extension=args.extension,
            records_per_file=args.recordsperfile
        )
    else:
        log.debug('Writing output to stdout')
        writer = outputs.StdOutWriter()

    if args.template:
        log.debug('Using template from specified file: %s', args.template)
        return outputs.RecordLevelOutput(engines.load(args.template), writer)

    if args.format:
        log.debug('Using %s formatter for output', args.format)
        formatter = outputs.FormatProcessor(args.format)
        return outputs.RecordLevelOutput(formatter, writer)

    # default
    return outputs.SingleFieldOutput(writer, args.printkey)


def _load_spec(args):
    spec_path = args.spec
    inline = args.inline
    if spec_path is None and inline is None:
        raise SpecException('One of --spec <spec path> or --inline "<spec string>" must be specified')
    if spec_path and inline:
        raise SpecException('Only one of --spec <spec path> or --inline "<spec string>" must be specified')
    if inline:
        return _parse_spec_string(inline)
    with open(spec_path, 'r') as handle:
        log.debug('Attempting to load spec as JSON')
        try:
            return json.load(handle)
        except json.decoder.JSONDecodeError:
            log.debug('Spec is not Valid JSON')
            pass
    # not JSON, try yaml
    with open(spec_path, 'r') as handle:
        log.debug('Attempting to load spec as YAML')
        spec = yaml.load(handle, Loader=yaml.FullLoader)
    if not isinstance(spec, dict):
        raise SpecException('Unable to load spec from path: %s, Please verify it is valid JSON or YAML', spec_path)
    return spec


def _parse_spec_string(inline):
    try:
        return json.loads(inline)
    except json.decoder.JSONDecodeError:
        log.debug('Spec is not Valid JSON')
        pass
    # not JSON, try yaml
    log.debug('Attempting to load spec as YAML')
    spec = yaml.load(inline, Loader=yaml.FullLoader)
    if not isinstance(spec, dict):
        raise SpecException('Unable to load spec from string: %s, Please verify it is valid JSON or YAML', inline)
    return spec


def _configure_logging(args):
    for name in dataspec.registry.logging.get_all():
        configure_function = dataspec.registry.logging.get(name)
        configure_function(args.log_level)


if __name__ == '__main__':
    # this activates the decorators, so they will be discoverable
    # cannot use * import due to pyinstaller not recognizing modules as being used
    from dataspec.type_handlers import combine
    from dataspec.type_handlers import range_handler
    from dataspec.type_handlers import select_list_subset
    from dataspec.type_handlers import weighted_ref
    from dataspec.type_handlers import uuid_handler
    from dataspec.type_handlers import ip_handler
    from dataspec.type_handlers import date_handler
    from dataspec.type_handlers import csv_handler
    from dataspec.type_handlers import geo_handler
    from dataspec.type_handlers import nested_handler
    from dataspec.type_handlers import char_class_handler
    from dataspec.type_handlers import unicode_range
    import dataspec.preprocessor
    import dataspec.logging_handler

    main()
